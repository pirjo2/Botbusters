{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vbsKHW6v9D7",
        "outputId": "50471085-a2b6-40ac-a58a-c40892d9ea04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#hannes oli siin, kas teised n√§evad seda?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Network Science/project/bot_detection_data.csv')"
      ],
      "metadata": {
        "id": "CRAkQJy7wC7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "dd8abb9f-9c47-421d-e9ae-00501c87e4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Network Science/project/bot_detection_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5ed4df20d75b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Network Science/project/bot_detection_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Network Science/project/bot_detection_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "70SNbMFMwVJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "tGmwaYsyxJ7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "hWYetdfyxH4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "N9OonIEOxbgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data description\n",
        "There is total of 11 columns and 50 000 rows of data. Each row represents 1 tweet along with its info. The data takes up about 3.9+ MB of memory.\n",
        "### Columns and description:\n",
        "Info was gotten from the Readme.md file from the dataset origin\n",
        "- User ID: Unique identifier for each user in the dataset.\n",
        "- Username: The username associated with the user.\n",
        "- Tweet: The text content of the tweet.\n",
        "- Retweet Count: The number of times the tweet has been retweeted.\n",
        "- Mention Count: The number of mentions in the tweet.\n",
        "- Follower Count: The number of followers the user has.\n",
        "- Verified: A boolean value indicating whether the user is verified or not.\n",
        "- Bot Label: A label indicating whether the user is a bot (1) or not (0).\n",
        "- Location: The location associated with the user.\n",
        "- Created At: The date and time when the tweet was created.\n",
        "- Hashtags: The hashtags associated with the tweet."
      ],
      "metadata": {
        "id": "R8oqhLc0xtcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some Nan values in the Hastags columns, but these are not errors in our case, these describe a tweet, where the user did not select/write any hashtags. Thus these Nan values are not removed."
      ],
      "metadata": {
        "id": "dsJkGqO8yKHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the data describe function, we can see that the median tweet got 50 retweets. The std is 29 for retweet count (p√§rast t√§psuustada!!). The minimum number of retweet count is 0 and the maximum is 100.\n",
        "- The median tweet got 2.5 mentions. The std is 1.7 for mention count. The minimum number of mention count is 0 and the maximum is 5.\n",
        "- The median tweet user has got 4988.6 followers. The std is 2878.7 for follower count. The minimum number of follower count is 0 and the maximum is 10000."
      ],
      "metadata": {
        "id": "hHtdrMPp0qxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Bot Label'].value_counts()"
      ],
      "metadata": {
        "id": "WohIWUuD14tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is total of 25018 tweets by bots and 24982 tweets by not bots, aka real human tweets. It can be summarised as 50% tweets by bots and 50% tweets by humans."
      ],
      "metadata": {
        "id": "MikIJN905kzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['User ID'].unique().size"
      ],
      "metadata": {
        "id": "Ys5RFVCN5kEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Username'].unique().size"
      ],
      "metadata": {
        "id": "3uLiI9SP6_bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the number of unique user IDs is the same as the total number of tweets in our dataset, this means that we have only 1 tweet from each user. This leaves out a possability of using the number of tweets per user. As the number of unique usernames is smaller than the total number of tweets, it might seem as we have many tweets per user, but this turned out to be not true based on the info gotten from the number of unique user IDs."
      ],
      "metadata": {
        "id": "wp9x6kFl6vPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Verified'].value_counts()"
      ],
      "metadata": {
        "id": "HoTnjLSN77Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a total of 25004 tweets from verified accounts and 24996 tweets from not verified accounts. To summarize, half of the tweets came from a verified account, the other half came from a not verified account."
      ],
      "metadata": {
        "id": "tk49v3lY8Hly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets post a hypothesis, most of the tweets that came from a not verified account, are from bots."
      ],
      "metadata": {
        "id": "zQJGpNvC8lv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['Verified'] == False) & (df['Bot Label'] == 1)].shape"
      ],
      "metadata": {
        "id": "9QBkqaXa8yhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turns out, 12540 tweets did fit our hypothesis, this is half of the 25018 bots we wanted to catch, this alone will not be enough, but might help us later."
      ],
      "metadata": {
        "id": "1R913KW09Pmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets have a look at where are the tweets coming from."
      ],
      "metadata": {
        "id": "hdYDs8Gd_dPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Location'].value_counts()"
      ],
      "metadata": {
        "id": "iKwIcLcu-b-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_counts = df['Location'].value_counts()\n",
        "location_counts.shape"
      ],
      "metadata": {
        "id": "cmij83Z_AL6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see how many of the most popular location tweets are from bots?"
      ],
      "metadata": {
        "id": "fNoQ7cWe_hxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['Location'] == 'South Michael') & (df['Bot Label'] == 1)].shape"
      ],
      "metadata": {
        "id": "JwiACpl5-ox-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Less than half this time. Lets look how many of the tweets came from places where the location count is 1."
      ],
      "metadata": {
        "id": "5GzFbgMU_nmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df[df['Location'].isin(location_counts[location_counts == 1].index)]\n",
        "filtered_df.shape"
      ],
      "metadata": {
        "id": "DFD_9a78_42A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df[(filtered_df['Bot Label'] == 1)].shape"
      ],
      "metadata": {
        "id": "RKy657Nu_wFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a total of 16295 tweets that have a location with the count of 1. Around half of these are from bots. It looks like the bots tweets are evenly distributed with locations."
      ],
      "metadata": {
        "id": "llQVtrSDBKD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create bins for retweet counts\n",
        "bin_edges = np.arange(0, 101, 10)\n",
        "\n",
        "# Create a histogram for bot tweets (Bot Label = 1)\n",
        "bot_tweets = df[df['Bot Label'] == 1]['Retweet Count']\n",
        "plt.hist(bot_tweets, bins=bin_edges, alpha=0.5, label='Bot', color='red')\n",
        "\n",
        "# Create a histogram for non-bot tweets (Bot Label = 0)\n",
        "non_bot_tweets = df[df['Bot Label'] == 0]['Retweet Count']\n",
        "plt.hist(non_bot_tweets, bins=bin_edges, alpha=0.5, label='Non-Bot', color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Retweet Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Retweet Counts for Bot and Non-Bot Tweets (0-100)')\n",
        "plt.xticks(bin_edges)  # Set x-axis ticks to match bins\n",
        "plt.legend()\n",
        "\n",
        "# Display the graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLr7-l2bD3bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Create bins for retweet counts 0-5\n",
        "bins = range(0, 6)  # Bins: 0, 1, 2, 3, 4, 5\n",
        "\n",
        "# Create a histogram for bot tweets (Bot Label = 1)\n",
        "bot_tweets = df[df['Bot Label'] == 1]['Mention Count']\n",
        "plt.hist(bot_tweets, bins=bins, alpha=0.5, label='Bot', color='red')\n",
        "\n",
        "# Create a histogram for non-bot tweets (Bot Label = 0)\n",
        "non_bot_tweets = df[df['Bot Label'] == 0]['Mention Count']\n",
        "plt.hist(non_bot_tweets, bins=bins, alpha=0.5, label='Non-Bot', color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Mention Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Mention Counts for Bot and Non-Bot Tweets (0-5)')\n",
        "plt.xticks(bins)  # Set x-axis ticks to match bins\n",
        "plt.legend()\n",
        "\n",
        "# Display the graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qjh98bTmEIwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create bins for retweet counts\n",
        "bin_edges = np.arange(0, 1001, 50)\n",
        "\n",
        "# Create a histogram for bot tweets (Bot Label = 1)\n",
        "bot_tweets = df[df['Bot Label'] == 1]['Follower Count']\n",
        "plt.hist(bot_tweets, bins=bin_edges, alpha=0.5, label='Bot', color='red')\n",
        "\n",
        "# Create a histogram for non-bot tweets (Bot Label = 0)\n",
        "non_bot_tweets = df[df['Bot Label'] == 0]['Follower Count']\n",
        "plt.hist(non_bot_tweets, bins=bin_edges, alpha=0.5, label='Non-Bot', color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Follower Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Follower Counts for Bot and Non-Bot Tweets (0-100)')\n",
        "plt.xticks(bin_edges)  # Set x-axis ticks to match bins\n",
        "plt.legend()\n",
        "\n",
        "# Display the graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Enbz_2UFPcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    user_id = row['User ID']\n",
        "    user_label = row['Bot Label']\n",
        "    user_followers = row['Follower Count']\n",
        "\n",
        "    # Add the node if not already present\n",
        "    if not G.has_node(user_id):\n",
        "        G.add_node(user_id,\n",
        "                   label=row['Username'],\n",
        "                   is_bot=user_label,\n",
        "                   followers=user_followers)\n",
        "\n",
        "    # If mentions exist, create edges\n",
        "    mentions = row['Tweet'].split('@')[1:]  # crude way to extract mentions\n",
        "    for mention in mentions:\n",
        "        mentioned_user = mention.split()[0].strip(',:.!?')  # clean up username\n",
        "        G.add_edge(user_id, mentioned_user)\n"
      ],
      "metadata": {
        "id": "dMdxRSmXrFjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = ['red' if G.nodes[n]['is_bot'] == 1 else 'blue' for n in G.nodes]\n",
        "sizes = [G.nodes[n]['followers'] / 10 + 10 for n in G.nodes]  # scale size\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "pos = nx.spring_layout(G, k=0.1)  # Force-directed layout\n",
        "\n",
        "nx.draw(G, pos, node_color=colors, node_size=sizes, with_labels=False, edge_color='gray', alpha=0.6)\n",
        "plt.title('User Interaction Network: Bots vs Humans')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TxSkR2Gkrjt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load data\n",
        "#df = pd.read_csv('your_data.csv')\n",
        "df2 = df.copy()\n",
        "# Basic cleaning\n",
        "df2['verified'] = df['Verified'].astype(int)\n",
        "df2['tweet_length'] = df['Tweet'].astype(str).apply(len)\n",
        "df2['hashtag_count'] = df['Hashtags'].astype(str).apply(lambda x: len(x.split(',')) if x != 'nan' else 0)\n",
        "\n",
        "# Extract time features\n",
        "df2['created_at'] = pd.to_datetime(df['Created At'])\n",
        "df2['tweet_hour'] = df2['created_at'].dt.hour\n",
        "df2['tweet_weekday'] = df2['created_at'].dt.weekday\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=100)\n",
        "tfidf_matrix = vectorizer.fit_transform(df2['Tweet'].astype(str))\n",
        "\n",
        "# Combine features\n",
        "X = pd.concat([\n",
        "    df2[['Follower Count', 'verified', 'Retweet Count', 'Mention Count', 'tweet_length', 'hashtag_count', 'tweet_hour', 'tweet_weekday']].reset_index(drop=True),\n",
        "    pd.DataFrame(tfidf_matrix.toarray())\n",
        "], axis=1)\n",
        "\n",
        "X.columns = X.columns.astype(str)  #\n",
        "\n",
        "y = df2['Bot Label']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "UhZS64joNjrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "pv_ZNgVKPayx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "indices = np.argsort(importances)[-8:]  # top 10\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Top 10 Feature Importances\")\n",
        "plt.barh(range(len(indices)), importances[indices], align=\"center\")\n",
        "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "coq_W1NGPd_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "BttFBmhIPxc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "IifQIbilP0G1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}